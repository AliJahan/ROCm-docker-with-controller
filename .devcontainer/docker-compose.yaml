version: "3.8"

services:
  experiment-runner:
    image: experiment-runner-ubuntu20.04:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.experiment-runner-ubuntu20.04
    stdin_open: true
    tty: true
    volumes:
      - ../:/workspace
    working_dir: /workspace
    command: /bin/bash
    network_mode: "host"

  rocm-ubuntu: # rocm-ubuntu: rocm-5.7.1 installed on ubuntu 20.04
    image: rocm5.7.1-ubuntu20.04:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.rocm5.7.1-ubuntu20.04
    stdin_open: true
    tty: true
    volumes:
      - ../:/workspace
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
  remote-rocm-ubuntu: # ROCR-Runtime modified to control cumasking remotely
    image: remote-rocm-ubuntu:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.remote-rocm-ubuntu
    stdin_open: true
    tty: true
    volumes:
      - ../:/workspace
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
    # ports:
    #   - "9090:9090"
    network_mode: "host"
  tvm-remote-rocm-ubuntu: # tvm-unity and pytorch 2.1 installed on remotely controllable (cumasking) rocm
    image: tvm-remote-rocm-ubuntu:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.tvm-torch-remote-rocm
    stdin_open: true
    tty: true
    volumes:
      - ../:/workspace
      - ../dist:/mlc-llm/dist
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
    # ports:
    #   - "9090:9090"
    network_mode: "host"
  Inference-Server: # contains gpu workloads on remotely controllable rocm
    image: Inference-Server-remote-rocm-ubuntu:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.gpu-workloads-remote-rocm
    stdin_open: true
    tty: true
    environment:
      - WORKLOAD=Inference-Server
      - CONTROLLER_IP=${CONTROLLER_IP}
      - CONTROLLER_PORT=${CONTROLLER_PORT}
    volumes:
      - ../:/workspace
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
    # ports:
    #   - "9090:9090"
    network_mode: "host"
  miniMDock: # contains gpu workloads on remotely controllable rocm
    image: miniMDock-remote-rocm-ubuntu:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.gpu-workloads-remote-rocm
    stdin_open: true
    tty: true
    environment:
      - WORKLOAD=miniMDock
      - CONTROLLER_IP=${CONTROLLER_IP}
      - CONTROLLER_PORT=${CONTROLLER_PORT}
    volumes:
      - ../:/workspace
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
    # ports:
    #   - "9090:9090"
    network_mode: "host"
  power-broadcaster: 
    image: power-broadcaster-rocm:latest
    build:
      context: ..
      dockerfile: ./.devcontainer/Dockerfile.power-broadcaster
    stdin_open: true
    tty: true
    volumes:
      - ../:/workspace
    devices:
      - "/dev/kfd:/dev/dri"
    privileged: true
    working_dir: /workspace
    command: /bin/bash
    profiles: ["amd"]
    network_mode: "host"